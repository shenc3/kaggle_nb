{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The outline of this notebook is as follows:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA) and Wordclouds** - Analyzing the data by generating simple statistics such word frequencies over the different authors as well as plotting some wordclouds (with image masks).\n",
    "2. **Natural Language Processing (NLP) with NLTK (Natural Language Toolkit)** - Introducing basic text processing methods such as tokenizations, stop word removal, stemming and vectorizing text via term frequencies (TF) as well as the inverse document frequencies (TF-IDF)\n",
    "3. **Topic Modelling with LDA and NNMF** - Implementing the two topic modelling techniques of Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from collections import Counter\n",
    "from scipy.misc import imread\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading in the training data with Pandas\n",
    "train = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Authors and their works EDA¶\n",
    "首先，观察一下数据集的前几行，它会告诉我们数据集的结构，以及作者具体是谁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics of the training set\n",
    "我们能对数据集的基础统计信息进行可视化，比如每个作家的语料分布。\n",
    "为完成可视化，会调用Plot.ly可视化包，并用它完成一些简单的bar plot。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "marker": {
          "color": [
           7900,
           6044,
           5635
          ],
          "colorscale": "Jet"
         },
         "text": "Text entries attributed to Author",
         "type": "bar",
         "x": [
          "Edgar Allen Poe",
          "HP Lovecraft",
          "Mary Shelley"
         ],
         "y": [
          7900,
          6044,
          5635
         ]
        }
       ],
       "layout": {
        "title": "Target variable distribution"
       }
      },
      "text/html": [
       "<div id=\"e2501fd5-0257-4ba2-8f86-daa05ef0b08f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e2501fd5-0257-4ba2-8f86-daa05ef0b08f\", [{\"x\": [\"Edgar Allen Poe\", \"HP Lovecraft\", \"Mary Shelley\"], \"type\": \"bar\", \"marker\": {\"color\": [7900, 6044, 5635], \"colorscale\": \"Jet\"}, \"text\": \"Text entries attributed to Author\", \"y\": [7900, 6044, 5635]}], {\"title\": \"Target variable distribution\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\n",
    "data = [go.Bar(\n",
    "            x = train.author.map(z).unique(),\n",
    "            y = train.author.value_counts().values,\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = train.author.value_counts().values\n",
    "                        ),\n",
    "            text='Text entries attributed to Author'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Target variable distribution'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = train['text'].str.split(expand=True).unstack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "marker": {
          "color": [
           17059,
           12615,
           10382,
           10359,
           8787,
           6440,
           5988,
           5037,
           4324,
           4207,
           3802,
           3528,
           3422,
           3247,
           3227,
           3203,
           3048,
           2925,
           2758,
           2725,
           2533,
           2521,
           2285,
           2268,
           2121,
           2118,
           2089,
           2015,
           2007,
           1953,
           1691,
           1679,
           1644,
           1500,
           1428,
           1395,
           1373,
           1372,
           1347,
           1308,
           1259,
           1215,
           1202,
           1178,
           1134,
           1104,
           1100,
           1036,
           992,
           957,
           954,
           929,
           925,
           924,
           915,
           901,
           880,
           873,
           863,
           861,
           829,
           823,
           814,
           751,
           713,
           707,
           701,
           648,
           645,
           645,
           638,
           624,
           623,
           600,
           595,
           592,
           576,
           575,
           574,
           565,
           550,
           545,
           539,
           534,
           532,
           531,
           527,
           527,
           526,
           503,
           502,
           497,
           493,
           493,
           491,
           488,
           486,
           482
          ],
          "colorscale": "Jet"
         },
         "text": "Word counts",
         "type": "bar",
         "x": [
          "and",
          "to",
          "I",
          "a",
          "in",
          "was",
          "that",
          "my",
          "had",
          "with",
          "his",
          "as",
          "he",
          "it",
          "for",
          "which",
          "not",
          "at",
          "from",
          "by",
          "is",
          "but",
          "on",
          "be",
          "The",
          "were",
          "have",
          "me",
          "this",
          "her",
          "all",
          "or",
          "an",
          "no",
          "you",
          "so",
          "been",
          "one",
          "we",
          "upon",
          "could",
          "its",
          "would",
          "when",
          "they",
          "their",
          "more",
          "him"
         ],
         "y": [
          17059,
          12615,
          10382,
          10359,
          8787,
          6440,
          5988,
          5037,
          4324,
          4207,
          3802,
          3528,
          3422,
          3247,
          3227,
          3203,
          3048,
          2925,
          2758,
          2725,
          2533,
          2521,
          2285,
          2268,
          2121,
          2118,
          2089,
          2015,
          2007,
          1953,
          1691,
          1679,
          1644,
          1500,
          1428,
          1395,
          1373,
          1372,
          1347,
          1308,
          1259,
          1215,
          1202,
          1178,
          1134,
          1104,
          1100,
          1036
         ]
        }
       ],
       "layout": {
        "title": "Top 50 (Uncleaned) Word frequencies in the training dataset"
       }
      },
      "text/html": [
       "<div id=\"9d3aba3c-f3ab-4b64-b584-592e382952bb\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9d3aba3c-f3ab-4b64-b584-592e382952bb\", [{\"x\": [\"and\", \"to\", \"I\", \"a\", \"in\", \"was\", \"that\", \"my\", \"had\", \"with\", \"his\", \"as\", \"he\", \"it\", \"for\", \"which\", \"not\", \"at\", \"from\", \"by\", \"is\", \"but\", \"on\", \"be\", \"The\", \"were\", \"have\", \"me\", \"this\", \"her\", \"all\", \"or\", \"an\", \"no\", \"you\", \"so\", \"been\", \"one\", \"we\", \"upon\", \"could\", \"its\", \"would\", \"when\", \"they\", \"their\", \"more\", \"him\"], \"type\": \"bar\", \"marker\": {\"color\": [17059, 12615, 10382, 10359, 8787, 6440, 5988, 5037, 4324, 4207, 3802, 3528, 3422, 3247, 3227, 3203, 3048, 2925, 2758, 2725, 2533, 2521, 2285, 2268, 2121, 2118, 2089, 2015, 2007, 1953, 1691, 1679, 1644, 1500, 1428, 1395, 1373, 1372, 1347, 1308, 1259, 1215, 1202, 1178, 1134, 1104, 1100, 1036, 992, 957, 954, 929, 925, 924, 915, 901, 880, 873, 863, 861, 829, 823, 814, 751, 713, 707, 701, 648, 645, 645, 638, 624, 623, 600, 595, 592, 576, 575, 574, 565, 550, 545, 539, 534, 532, 531, 527, 527, 526, 503, 502, 497, 493, 493, 491, 488, 486, 482], \"colorscale\": \"Jet\"}, \"text\": \"Word counts\", \"y\": [17059, 12615, 10382, 10359, 8787, 6440, 5988, 5037, 4324, 4207, 3802, 3528, 3422, 3247, 3227, 3203, 3048, 2925, 2758, 2725, 2533, 2521, 2285, 2268, 2121, 2118, 2089, 2015, 2007, 1953, 1691, 1679, 1644, 1500, 1428, 1395, 1373, 1372, 1347, 1308, 1259, 1215, 1202, 1178, 1134, 1104, 1100, 1036]}], {\"title\": \"Top 50 (Uncleaned) Word frequencies in the training dataset\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [go.Bar(\n",
    "            x = all_words.index.values[2:50],\n",
    "            y = all_words.values[2:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = all_words.values[2:100]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 (Uncleaned) Word frequencies in the training dataset'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything odd about the words that appear in this word frequency plot? Do these words actually tell us much about the themes and concepts that Mary Shelley wants to portray to the reader in her stories?\n",
    "\n",
    "These words are all so commonly occuring words which you could find just anywhere else. Not just in spooky stories and novels by our three authors but also in newspapers, kid book, religious texts - really almost every other english text. Therefore we must find some way to preprocess our dataset first to strip out all these commonly occurring words which do not bring much to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds to visualise each author's work\n",
    "One very handy visualization tool for a data scientist when it comes to any sort of natural language processing is plotting \"Word Cloud\". A word cloud (as the name suggests) is an image that is made up of a mixture of distinct words which may make up a text or book and where the size of each word is proportional to its word frequency in that text (number of times the word appears). Here instead of dealing with an actual book or text, our words can simply be taken from the column \"text\"\n",
    "\n",
    "#### Store the text of each author in a Python list\n",
    "We first create three different python lists that store the texts of Edgar Allen Poe, HP Lovecraft and Mary Shelley respectively as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eap = train[train.author==\"EAP\"][\"text\"].values\n",
    "hpl = train[train.author==\"HPL\"][\"text\"].values\n",
    "mws = train[train.author==\"MWS\"][\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c2958a9c3fbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Next to create our WordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Next to create our WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Natural Language Processing\n",
    "\n",
    "在几乎所有的你能碰到的NLP(探索计算机和人类语言之间交互关系的领域)任务中（topic modeling, word clustering, document-text classification等），工作人员一般都会经历如下的几个数据预处理的阶段，为的是将输入的原始文本转化为模型或者机器能够理解的数据。期望把原始的文本数据交给一个随机森林模型，然后让它立即预测出结果是不现实的。\n",
    "\n",
    "文本预处理可以分成以下几个步骤：\n",
    "\n",
    "1. **Tokenization** - 分词\n",
    "2. **Stopwords** - 丢掉哪些出现得过于频繁的词，以至于它们的出现频率对预测相关文本毫无帮助（此外，还经常将那些出现频率过低的词也丢弃）\n",
    "3. **Stemming** - 组合有变体的词为同一个父类词组(parent word)，因为它们表达的是同一个意思\n",
    "4. **Vectorization** - 将文本转化为向量格式。最简单的方法是著名的词袋（bag-of-words）方法，通过它可以为每个语料中对文档或文本创建一个矩阵。在最简单的形式中，这个矩阵存储了词频信息，通常称之为原始文本的向量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. 分词(Tokenization)\n",
    "The concept of tokenization is the act of taking a sequence of characters (think of Python strings) in a given document and dicing it up into its individual constituent pieces, which are the eponymous \"tokens\" of this method. One could loosely think of them as singular words in a sentence. One could naively implement the \"split( )\" method on a string which separates it into a python list based on the identifier in the argument. It is actually not that trivial to\n",
    "\n",
    "可以简单的使用split来完成分词，用split()方法将字符串拆分成一个个单独的词，但实际上并没有这么简单。\n",
    "\n",
    "Here we split the first sentence of the text in the training data just on a space as follows:\n",
    "\n",
    "我们将训练文本中的第一句话进行分词：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\n",
      "==========================================================================================\n",
      "['This', 'process,', 'however,', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon;', 'as', 'I', 'might', 'make', 'its', 'circuit,', 'and', 'return', 'to', 'the', 'point', 'whence', 'I', 'set', 'out,', 'without', 'being', 'aware', 'of', 'the', 'fact;', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall.']\n"
     ]
    }
   ],
   "source": [
    "# Storing the first text element as a string\n",
    "first_text = train.text.values[0]\n",
    "print(first_text)\n",
    "print(\"=\"*90)\n",
    "print(first_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，正如你能看到的那样，单纯的用split方法有时候显得不够准确，例如第二个分词\"process,\"，逗号(\",\")被包含进词组中，但其实这并不是我们所需要的。\n",
    "理想情况是，我们希望将逗号和词语分开，而单纯使用python语句来实现会很麻烦，而这时候NLTK包会派上用场。\n",
    "可以使用word_tokenize()方法，将词语和标点符号都分开为单独的元素：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'I', 'might', 'make', 'its', 'circuit', ',', 'and', 'return', 'to', 'the', 'point', 'whence', 'I', 'set', 'out', ',', 'without', 'being', 'aware', 'of', 'the', 'fact', ';', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '.']\n"
     ]
    }
   ],
   "source": [
    "first_text_list = nltk.word_tokenize(first_text)\n",
    "print(first_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. 移除停用词(Stopword Removal)\n",
    "正如上文提到的那样，停用词指的是那些出现频率过高以至于对预测或者学习过程贡献甚微的词语。停用词包括像\"to\" 或者 \"the\"这样的词，所以我们需要在预处理过程中将其去掉。\n",
    "NLTK中预先定义了一个包含153个英文停用词的list可供使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用一个列表生成式（list comprehension）来将停用词从我们的分词结果中过滤出去："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.']\n",
      "==========================================================================================\n",
      "Length of original list: 48 words\n",
      "Length of list after stopwords removal: 28 words\n"
     ]
    }
   ],
   "source": [
    "first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\n",
    "print(first_text_list_cleaned)\n",
    "print(\"=\"*90)\n",
    "print(\"Length of original list: {0} words\\n\"\n",
    "      \"Length of list after stopwords removal: {1} words\"\n",
    "      .format(len(first_text_list), len(first_text_list_cleaned)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. 词形规范化：词干提取和词形还原 （Stemming and Lemmatization）\n",
    "NLP中去除停用词后的下一个步骤是词干提取（Stemming）。这一步的工作尝试着将那些具有相同意思的词合并为同一个词根。例如当我们有\"running\", \"runs\"和 \"run\"，将会把这3个不同的词合并为run。尽管这将会损失时态信息。\n",
    "\n",
    "NLTK提供了多种stemmer方法，包括Porter stemming algorithm, the lancaster stemmer 以及 the Snowball stemmer。\n",
    "\n",
    "在下面的例子中，会从创建一个stemmer实例开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们就能观察stemmer是否能从词组（running, runs, run）中提取出词干（run）来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of running is: run\n",
      "The stemmed form of runs is: run\n",
      "The stemmed form of run is: run\n"
     ]
    }
   ],
   "source": [
    "print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\n",
    "print(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\n",
    "print(\"The stemmed form of run is: {}\".format(stemmer.stem(\"run\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the stemmer has successfully reduced the given words above into a base form and this will be most in helping us reduce the size of our dataset of words when we come to learning and classification tasks.\n",
    "\n",
    "However there is one flaw with stemming and that is the fact that the process involves quite a crude heuristic in chopping off the ends of words in the hope of reducing a particular word into a human recognizable base form. Therefore this process does not take into account vocabulary or word forms when collapsing words as this example will illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed form of leaves is: leav\n"
     ]
    }
   ],
   "source": [
    "print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization to the rescue\n",
    "Therefore we turn to another that we could use in lieu of stemming. This method is called lemmatization which aims to achieve the same effect as the former method. However unlike a stemmer, lemmatizing the dataset aims to reduce words based on an actual dictionary or vocabulary (the Lemma) and therefore will not chop off words into stemmed forms that do not carry any lexical meaning. Here we can utilize NLTK once again to initialize a lemmatizer (WordNet variant) and inspect how it collapses words as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized form of leaves is: leaf\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "print(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
