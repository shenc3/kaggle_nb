{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉验证策略\n",
    "\n",
    "[Scikit-learn validator](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)\n",
    "\n",
    "[Cross-validation for time series](https://robjhyndman.com/hyndsight/tscv/)\n",
    "\n",
    "[Ordered cross-validation](https://github.com/MaxHalford/xam/blob/master/docs/model-selection.md#ordered-cross-validation)\n",
    "\n",
    "[Cross-Validation Methodology Using '16 Golden Week](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/45266)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'tra': pd.read_csv('./data/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('./data/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('./data/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('./data/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('./data/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('./data/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('./data/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('./data/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "# 初始处理:\n",
    "#       1. 从tes数据id中提取air_store_id和visit_datetime\n",
    "#       2. 在HPG预订信息中匹配air_store_id\n",
    "#       2. 转换visit_datetime和reserve_datetime为时间格式\n",
    "print(\"Init.\")\n",
    "# 1.\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "# 2.\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "# 3.\n",
    "for k in data:\n",
    "    if 'visit_date' in data[k].columns:\n",
    "        data[k]['visit_date'] = pd.to_datetime(data[k]['visit_date'])\n",
    "    if 'visit_datetime' in data[k].columns:\n",
    "        data[k]['visit_date'] = pd.to_datetime(data[k]['visit_datetime'].str.split().str[0])\n",
    "        data[k].drop('visit_datetime', axis=1, inplace=True)\n",
    "    if 'reserve_datetime' in data[k].columns:\n",
    "        data[k]['reserve_date'] = pd.to_datetime(data[k]['reserve_datetime'].str.split().str[0])\n",
    "        data[k].drop('reserve_datetime', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义特征抽取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count / ratio\n",
    "# data['tra'].groupby('air_store_id')['visitors'].sum()\n",
    "def time_feats(df, dt_col='visit_date'):\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    df.loc[:, 'year'] = df[dt_col].dt.year\n",
    "    df.loc[:, 'dow'] = df[dt_col].dt.dayofweek\n",
    "    df.loc[:, 'month'] = df[dt_col].dt.month\n",
    "    df.loc[:, 'season'] = df[dt_col].dt.quarter\n",
    "    df.loc[:, 'week'] = df[dt_col].dt.weekofyear\n",
    "    df.loc[:, 'doy'] = df[dt_col].dt.dayofyear\n",
    "    df.loc[:, 'date_int'] = df[dt_col].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "\n",
    "# 增加时间特征\n",
    "time_feats(data['tra'])\n",
    "time_feats(data['tes'])\n",
    "\n",
    "# 匹配genre和area信息\n",
    "data['tra'] = data['tra'].merge(data['as'][['air_store_id', 'air_genre_name', 'air_area_name']], how='left', on='air_store_id')\n",
    "data['tes'] = data['tes'].merge(data['as'][['air_store_id', 'air_genre_name', 'air_area_name']], how='left', on='air_store_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PART1. Count & Agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store\n",
    "# 1. overall\n",
    "store_agg_all = data['tra'].groupby('air_store_id')['visitors'].agg([np.sum, np.mean, np.std])\n",
    "\n",
    "# 2. monthly data\n",
    "store_cnt_monthly = data['tra'].groupby(['air_store_id', 'year', 'month'])['visitors'].sum().to_frame().reset_index()  # 过程数据\n",
    "store_mean_std_monthly = store_cnt_monthly.groupby('air_store_id')['visitors'].agg([np.mean, np.std])\n",
    "store_agg_monthly = store_cnt_monthly.groupby(['air_store_id', 'month'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "store_agg_monthly.columns = ['min_m', 'mean_m', 'median_m', 'max_m']\n",
    "store_agg_monthly.loc[:, 'ratio_m'] = store_agg_monthly['mean_m'] / store_mean_std_monthly['mean']\n",
    "\n",
    "# 3. weekly data\n",
    "store_cnt_weekly = data['tra'].groupby(['air_store_id', 'year', 'month', 'week'])['visitors'].sum().to_frame().reset_index()  # 过程数据\n",
    "store_mean_std_weekly = store_cnt_weekly.groupby('air_store_id')['visitors'].agg([np.mean, np.std])\n",
    "store_agg_weekly = store_cnt_weekly.groupby(['air_store_id', 'week'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "store_agg_weekly.columns = ['min_w', 'mean_w', 'median_w', 'max_w']\n",
    "store_agg_weekly.loc[:, 'ratio_w'] = store_agg_weekly['mean_w'] / store_mean_std_weekly['mean']\n",
    "\n",
    "# 4. dow data\n",
    "store_agg_dow = data['tra'].groupby(['air_store_id', 'dow'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max, np.std, np.size])\n",
    "store_agg_dow.columns = ['min_d', 'mean_d', 'median_d', 'max_d', 'std_d', 'len_d']\n",
    "store_agg_dow.loc[:, 'ratio_d'] = store_agg_dow['mean_d'] / store_agg_all['mean']\n",
    "\n",
    "# 5. penetration\n",
    "store_penetration = (data['tra'].groupby('air_store_id')['visitors'].sum() / data['tra']['visitors'].sum()).to_frame('penet_store')\n",
    "\n",
    "comment = '''\n",
    "store_agg_all                    --> <index> air_store_id,            <cols> sum, mean, std\n",
    "store_mean_std_monthly --> <index> air_store_id             <cols> mean, std\n",
    "store_agg_monthly          --> <index> air_store_id, month <cols> min_m, mean_m, median_m, max_m, ratio_m\n",
    "store_mean_std_weekly   --> <index> air_store_id,             <cols> mean, std\n",
    "store_agg_weekly            --> <index> air_store_id, week    <cols> min_w, mean_w, median_w, max_w, ratio_w\n",
    "store_agg_dow                --> <index> air_store_id, dow      <cols> min_d, mean_d, median_d, max_d, std_d, len_d, ratio_d\n",
    "store_penetration            --> <index> air_store_id               <cols> penet_store\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre\n",
    "# 1. overall\n",
    "genre_agg_all = data['tra'].groupby('air_genre_name')['visitors'].agg([np.sum, np.mean, np.std])\n",
    "\n",
    "# 2. monthly data\n",
    "genre_cnt_monthly = data['tra'].groupby(['air_genre_name', 'year', 'month'])['visitors'].sum().to_frame().reset_index()\n",
    "genre_mean_std_monthly = genre_cnt_monthly.groupby('air_genre_name')['visitors'].agg([np.mean, np.std])\n",
    "genre_agg_monthly = genre_cnt_monthly.groupby(['air_genre_name', 'month'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "genre_agg_monthly.columns = ['min_m', 'mean_m', 'median_m', 'max_m']\n",
    "genre_agg_monthly.loc[:, 'ratio_m'] = genre_agg_monthly['mean_m'] / genre_mean_std_monthly['mean']\n",
    "\n",
    "# 3. weekly data\n",
    "genre_cnt_weekly = data['tra'].groupby(['air_genre_name', 'year', 'month', 'week'])['visitors'].sum().to_frame().reset_index()\n",
    "genre_mean_std_weekly = genre_cnt_weekly.groupby('air_genre_name')['visitors'].agg([np.mean, np.std])\n",
    "genre_agg_weekly = genre_cnt_weekly.groupby(['air_genre_name', 'week'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "genre_agg_weekly.columns = ['min_w', 'mean_w', 'median_w', 'max_w']\n",
    "genre_agg_weekly.loc[:, 'ratio_w'] = genre_agg_weekly['mean_w'] / genre_mean_std_weekly['mean']\n",
    "\n",
    "# 4. dow data\n",
    "genre_agg_dow = data['tra'].groupby(['air_genre_name', 'dow'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max, np.std, np.size])\n",
    "genre_agg_dow.columns = ['min_d', 'mean_d', 'median_d', 'max_d', 'std_d', 'len_d']\n",
    "genre_agg_dow.loc[:, 'ratio_d'] = genre_agg_dow['mean_d'] / genre_agg_all['mean']\n",
    "\n",
    "# 5. penetration\n",
    "genre_penetration = (data['tra'].groupby('air_genre_name')['visitors'].sum() / data['tra']['visitors'].sum()).to_frame('penet_genre')\n",
    "\n",
    "comment = '''\n",
    "genre_agg_all                    --> <index> air_genre_name,              <cols> sum, mean, std\n",
    "genre_mean_std_monthly --> <index> air_genre_name               <cols> mean, std\n",
    "genre_agg_monthly          --> <index> air_genre_name, month   <cols> min_m, mean_m, median_m, max_m, ratio_m\n",
    "genre_mean_std_weekly   --> <index> air_genre_name,               <cols> mean, std\n",
    "genre_agg_weekly            --> <index> air_genre_name, week      <cols> min_w, mean_w, median_w, max_w, ratio_w\n",
    "genre_agg_dow                --> <index> air_genre_name, dow       <cols> min_d, mean_d, median_d, max_d, std_d, len_d, ratio_d\n",
    "genre_penetration            --> <index> air_genre_name                <cols> penet_genre\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Area\n",
    "# 1. overall\n",
    "area_agg_all = data['tra'].groupby('air_area_name')['visitors'].agg([np.sum, np.mean, np.std])\n",
    "\n",
    "# 2. monthly data\n",
    "area_cnt_monthly = data['tra'].groupby(['air_area_name', 'year', 'month'])['visitors'].sum().to_frame().reset_index()\n",
    "area_mean_std_monthly = area_cnt_monthly.groupby('air_area_name')['visitors'].agg([np.mean, np.std])\n",
    "area_agg_monthly = area_cnt_monthly.groupby(['air_area_name', 'month'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "area_agg_monthly.columns = ['min_m', 'mean_m', 'median_m', 'max_m']\n",
    "area_agg_monthly.loc[:, 'ratio_m'] = area_agg_monthly['mean_m'] / area_mean_std_monthly['mean']\n",
    "\n",
    "# 3. weekly data\n",
    "area_cnt_weekly = data['tra'].groupby(['air_area_name', 'year', 'month', 'week'])['visitors'].sum().to_frame().reset_index()\n",
    "area_mean_std_weekly = area_cnt_weekly.groupby('air_area_name')['visitors'].agg([np.mean, np.std])\n",
    "area_agg_weekly = area_cnt_weekly.groupby(['air_area_name', 'week'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "area_agg_weekly.columns = ['min_w', 'mean_w', 'median_w', 'max_w']\n",
    "area_agg_weekly.loc[:, 'ratio_w'] = area_agg_weekly['mean_w'] / area_mean_std_weekly['mean']\n",
    "\n",
    "# 4. dow data\n",
    "area_agg_dow = data['tra'].groupby(['air_area_name', 'dow'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max, np.std, np.size])\n",
    "area_agg_dow.columns = ['min_d', 'mean_d', 'median_d', 'max_d', 'std_d', 'len_d']\n",
    "area_agg_dow.loc[:, 'ratio_d'] = area_agg_dow['mean_d'] / area_agg_all['mean']\n",
    "\n",
    "# 5. penetration\n",
    "area_penetration = (data['tra'].groupby('air_area_name')['visitors'].sum() / data['tra']['visitors'].sum()).to_frame('penet_area')\n",
    "\n",
    "comment = '''\n",
    "area_agg_all                    --> <index> air_area_name,              <cols> sum, mean, std\n",
    "area_mean_std_monthly --> <index> air_area_name               <cols> mean, std\n",
    "area_agg_monthly          --> <index> air_area_name, month   <cols> min_m, mean_m, median_m, max_m, ratio_m\n",
    "area_mean_std_weekly   --> <index> air_area_name,               <cols> mean, std\n",
    "area_agg_weekly            --> <index> air_area_name, week      <cols> min_w, mean_w, median_w, max_w, ratio_w\n",
    "area_agg_dow                --> <index> air_area_name, dow       <cols> min_d, mean_d, median_d, max_d, std_d, len_d, ratio_d\n",
    "area_penetration            --> <index> air_area_name                <cols> penet_area\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# genre X area\n",
    "# data['tra']['air_area_name1'] = data['tra']['air_area_name'].apply(lambda x: x.split()[0].split('-')[0])\n",
    "# data['tra']['air_area_name2'] = data['tra']['air_area_name'].apply(lambda x: ' '.join(x.split()[:2]))\n",
    "\n",
    "# 1. overall count\n",
    "genre_area_cnt_all = data['tra'].groupby(['air_genre_name', 'air_area_name'])['visitors'].agg([np.sum, np.mean, np.std])\n",
    "\n",
    "# 2. monthly data\n",
    "genre_area_cnt_monthly = data['tra'].groupby(['air_genre_name', 'air_area_name', 'year', 'month'])['visitors'].sum().to_frame().reset_index()\n",
    "genre_area_mean_std_monthly = genre_area_cnt_monthly.groupby(['air_genre_name', 'air_area_name'])['visitors'].agg([np.mean, np.std])\n",
    "genre_area_agg_monthly = genre_area_cnt_monthly.groupby(['air_genre_name', 'air_area_name', 'month'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "genre_area_agg_monthly.columns = ['min_m', 'mean_m', 'median_m', 'max_m']\n",
    "\n",
    "# genre_area_agg_monthly.loc['ratio_m'] = genre_area_agg_monthly['mean_m'] / genre_area_mean_std_monthly['mean'] # 比率属性: 报错了\n",
    "\n",
    "# 3. weekly data\n",
    "genre_area_cnt_weekly = data['tra'].groupby(['air_genre_name', 'air_area_name', 'year', 'month', 'week'])['visitors'].sum().to_frame().reset_index()\n",
    "genre_area_mean_std_weekly = genre_area_cnt_weekly.groupby(['air_genre_name', 'air_area_name'])['visitors'].agg([np.mean, np.std])\n",
    "genre_area_agg_weekly = genre_area_cnt_weekly.groupby(['air_genre_name', 'air_area_name', 'week'])['visitors'].agg(\n",
    "    [np.min, np.mean, np.median, np.max])\n",
    "genre_area_agg_weekly.columns = ['min_w', 'mean_w', 'median_w', 'max_w']\n",
    "\n",
    "# 4. dow data\n",
    "genre_area_agg_dow = data['tra'].groupby(['air_genre_name', 'air_area_name', 'dow'])['visitors'].agg(\n",
    "                                      [np.min, np.mean, np.median, np.max, np.std, np.size])\n",
    "genre_area_agg_dow.columns = ['min_d', 'mean_d', 'median_d', 'max_d', 'std_d', 'len_d']\n",
    "# genre_area_agg_dow.loc[:, 'ratio_d'] = genre_area_agg_dow['mean_d'] / genre_area_cnt_all['mean']\n",
    "# NotImplementedError: merging with more than one level overlap on a multi-index is not implemented\n",
    "\n",
    "# 5. penetration\n",
    "genre_area_penetration = (data['tra'].groupby(['air_genre_name', 'air_area_name'])['visitors'].sum() / data['tra']['visitors'].sum()).to_frame('penet_ga')\n",
    "\n",
    "# WARNING: 多样性信息可以加上HPG系统的store信息，可以两边都用\n",
    "# 6. diversity\n",
    "genre_area_diver = data['tra'][['air_store_id', 'air_genre_name', 'air_area_name']].groupby([\n",
    "    'air_genre_name', 'air_area_name'])['air_store_id'].nunique().to_frame('store_num').reset_index()\n",
    "genre_num = data['tra'].groupby('air_genre_name')['air_store_id'].nunique()\n",
    "area_num = data['tra'].groupby('air_area_name')['air_store_id'].nunique()\n",
    "\n",
    "genre_area_diver = genre_area_diver.merge(area_num.to_frame(name='area_store_num'), how='left', left_on='air_area_name', right_index=True)\n",
    "genre_area_diver = genre_area_diver.merge(genre_num.to_frame(name='genre_store_num'), how='left', left_on='air_genre_name', right_index=True)\n",
    "genre_area_diver.loc[:, 'area_store_ratio'] = genre_area_diver['store_num'] / genre_area_diver['area_store_num']\n",
    "genre_area_diver.loc[:, 'genre_store_ratio'] = genre_area_diver['store_num'] / genre_area_diver['genre_store_num']\n",
    "genre_area_diver.set_index(['air_genre_name', 'air_area_name'], inplace=True)\n",
    "\n",
    "comment = '''\n",
    "genre_area_cnt_all                     --> <index> air_genre_name, air_area_name                <cols> sum, mean, std\n",
    "genre_area_mean_std_monthly --> <index> air_genre_name, air_area_name                <cols> mean, std\n",
    "genre_area_agg_monthly          --> <index> air_genre_name, air_area_name, month    <cols> min_m, mean_m, median_m, max_m\n",
    "genre_area_mean_std_weekly   --> <index> air_genre_name, air_area_name,               <cols> mean, std\n",
    "genre_area_agg_weekly            --> <index> air_genre_name, air_area_name, week      <cols> min_w, mean_w, median_w, max_w\n",
    "genre_area_agg_dow                --> <index> air_genre_name, air_area_name, dow        <cols> min_d, mean_d, median_d, max_d, std_d, len_d\n",
    "genre_area_penetration            --> <index> air_genre_name, air_area_name,                <cols> penet_ga\n",
    "genre_area_diver                       --> <index> air_genre_name, air_area_name,                <cols> store_num, area_store_num, genre_store_num\n",
    "                                                                                                                                                       area_store_ratio, genre_store_ratio\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>store_num</th>\n",
       "      <th>area_store_num</th>\n",
       "      <th>genre_store_num</th>\n",
       "      <th>area_store_ratio</th>\n",
       "      <th>genre_store_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_genre_name</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <th>Tōkyō-to Shibuya-ku Shibuya</th>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Bar/Cocktail</th>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Daimyō</th>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>79</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.088608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Hakata Ekimae</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.025316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hiroshima-ken Hiroshima-shi Kokutaijimachi</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.025316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hokkaidō Asahikawa-shi 6 Jōdōri</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>79</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.050633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           store_num  \\\n",
       "air_genre_name air_area_name                                           \n",
       "Asian          Tōkyō-to Shibuya-ku Shibuya                         2   \n",
       "Bar/Cocktail   Fukuoka-ken Fukuoka-shi Daimyō                      7   \n",
       "               Fukuoka-ken Fukuoka-shi Hakata Ekimae               2   \n",
       "               Hiroshima-ken Hiroshima-shi Kokutaijimachi          2   \n",
       "               Hokkaidō Asahikawa-shi 6 Jōdōri                     4   \n",
       "\n",
       "                                                           area_store_num  \\\n",
       "air_genre_name air_area_name                                                \n",
       "Asian          Tōkyō-to Shibuya-ku Shibuya                             58   \n",
       "Bar/Cocktail   Fukuoka-ken Fukuoka-shi Daimyō                          64   \n",
       "               Fukuoka-ken Fukuoka-shi Hakata Ekimae                   16   \n",
       "               Hiroshima-ken Hiroshima-shi Kokutaijimachi              23   \n",
       "               Hokkaidō Asahikawa-shi 6 Jōdōri                         13   \n",
       "\n",
       "                                                           genre_store_num  \\\n",
       "air_genre_name air_area_name                                                 \n",
       "Asian          Tōkyō-to Shibuya-ku Shibuya                               2   \n",
       "Bar/Cocktail   Fukuoka-ken Fukuoka-shi Daimyō                           79   \n",
       "               Fukuoka-ken Fukuoka-shi Hakata Ekimae                    79   \n",
       "               Hiroshima-ken Hiroshima-shi Kokutaijimachi               79   \n",
       "               Hokkaidō Asahikawa-shi 6 Jōdōri                          79   \n",
       "\n",
       "                                                           area_store_ratio  \\\n",
       "air_genre_name air_area_name                                                  \n",
       "Asian          Tōkyō-to Shibuya-ku Shibuya                         0.034483   \n",
       "Bar/Cocktail   Fukuoka-ken Fukuoka-shi Daimyō                      0.109375   \n",
       "               Fukuoka-ken Fukuoka-shi Hakata Ekimae               0.125000   \n",
       "               Hiroshima-ken Hiroshima-shi Kokutaijimachi          0.086957   \n",
       "               Hokkaidō Asahikawa-shi 6 Jōdōri                     0.307692   \n",
       "\n",
       "                                                           genre_store_ratio  \n",
       "air_genre_name air_area_name                                                  \n",
       "Asian          Tōkyō-to Shibuya-ku Shibuya                          1.000000  \n",
       "Bar/Cocktail   Fukuoka-ken Fukuoka-shi Daimyō                       0.088608  \n",
       "               Fukuoka-ken Fukuoka-shi Hakata Ekimae                0.025316  \n",
       "               Hiroshima-ken Hiroshima-shi Kokutaijimachi           0.025316  \n",
       "               Hokkaidō Asahikawa-shi 6 Jōdōri                      0.050633  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_area_diver.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#   1. 重命名\n",
    "#   2. 匹配对应值\n",
    "#   3. fillna\n",
    "\n",
    "for df, cols in (\n",
    "                    (store_agg_all,  ['sum_s', 'mean_s', 'std_s']),\n",
    "                    (store_mean_std_monthly, ['mean_s_mon', 'std_s_mon']),\n",
    "                    (store_agg_monthly, ['min_s_m', 'mean_s_m', 'median_s_m', 'max_s_m', 'ratio_s_m']),\n",
    "                    (store_mean_std_weekly, ['mean_s_week', 'std_s_week']),\n",
    "                    (store_agg_weekly, ['min_s_w', 'mean_s_w', 'median_s_w', 'max_s_w', 'ratio_s_w']),\n",
    "                    (store_agg_dow, ['min_s_d', 'mean_s_d', 'median_s_d', 'max_s_d', 'std_s_d', 'len_s_d', 'ratio_s_d']),\n",
    "                    (store_penetration, ['penet_store',]),\n",
    "                    (genre_agg_all, ['sum_g', 'mean_g', 'std_g']),\n",
    "                    (genre_mean_std_monthly, ['mean_g_mon', 'std_g_mon']),\n",
    "                    (genre_agg_monthly, ['min_g_m', 'mean_g_m', 'median_g_m', 'max_g_m', 'ratio_g_m']),\n",
    "                    (genre_mean_std_weekly, ['mean_g_week', 'std_g_week']),\n",
    "                    (genre_agg_weekly, ['min_g_w', 'mean_g_w', 'median_g_w', 'max_g_w', 'ratio_g_w']),\n",
    "                    (genre_agg_dow, ['min_g_d', 'mean_g_d', 'median_g_d', 'max_g_d', 'std_g_d', 'len_g_d', 'ratio_g_d']),\n",
    "                    (genre_penetration, ['penet_genre',]),\n",
    "                    (area_agg_all, ['sum_a', 'mean_a', 'std_a']),\n",
    "                    (area_mean_std_monthly, ['mean_a_mon', 'std_a_mon']),\n",
    "                    (area_agg_monthly, ['min_a_m', 'mean_a_m', 'median_a_m', 'max_a_m', 'ratio_a_m']),\n",
    "                    (area_mean_std_weekly, ['mean_a_week', 'std_a_week']),\n",
    "                    (area_agg_weekly, ['min_a_w', 'mean_a_w', 'median_a_w', 'max_a_w', 'ratio_a_w']),\n",
    "                    (area_agg_dow, ['min_a_d', 'mean_a_d', 'median_a_d', 'max_a_d', 'std_a_d', 'len_a_d', 'ratio_a_d']),\n",
    "                    (area_penetration, ['penet_area', ]),\n",
    "                    (genre_area_cnt_all, ['sum_ga', 'mean_ga', 'std_ga']),\n",
    "                    (genre_area_mean_std_monthly, ['mean_ga_mon', 'std_ga_mon']),\n",
    "                    (genre_area_agg_monthly, ['min_ga_m', 'mean_ga_m', 'median_ga_m', 'max_ga_m']),\n",
    "                    (genre_area_mean_std_weekly, ['mean_ga_week', 'std_ga_week']),\n",
    "                    (genre_area_agg_weekly, ['min_ga_w', 'mean_ga_w', 'median_ga_w', 'max_ga_w']),\n",
    "                    (genre_area_agg_dow, ['min_ga_d', 'mean_ga_d', 'median_ga_d', 'max_ga_d', 'std_ga_d', 'len_ga_d']),\n",
    "                    (genre_area_penetration, ['penet_ga',]),\n",
    "                    (genre_area_diver, ['store_num', 'area_store_num', 'genre_store_num', 'area_store_ratio', 'genre_store_ratio'])\n",
    "                ):\n",
    "    df.columns = cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART2. 时间 & Genre & Area & Holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part2\n",
    "# 为df加上时间标记: dow, month, year, date\n",
    "def time_feats(df, dt_col='visit_date'):\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col])\n",
    "    df.loc[:, 'year'] = df[dt_col].dt.year\n",
    "    df.loc[:, 'dow'] = df[dt_col].dt.dayofweek\n",
    "    df.loc[:, 'month'] = df[dt_col].dt.month\n",
    "    df.loc[:, 'season'] = df[dt_col].dt.quarter\n",
    "    df.loc[:, 'week'] = df[dt_col].dt.weekofyear\n",
    "    df.loc[:, 'doy'] = df[dt_col].dt.dayofyear\n",
    "    df.loc[:, 'date_int'] = df[dt_col].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "    return df\n",
    "\n",
    "# 预订数据特征\n",
    "# WARNING: 测试数据中没有reserve数据, 只有reserve_visit数据 \n",
    "def order_feats(order_data):\n",
    "    '''\n",
    "    Return:\n",
    "    -----\n",
    "    `air_store_id`, `visit_date`, ...\n",
    "    '''\n",
    "    order = order_data.copy()\n",
    "    # 创建新特征datetime_diff: 表示到店时间和预订时间的差值\n",
    "    order['reserve_date_diff'] = order.apply(\n",
    "        lambda r: (r['visit_date'] - r['reserve_date']).days,\n",
    "        axis=1)\n",
    "\n",
    "    # 以(air_store_id, visit_date)为分组计算预订时间差和预订人数的总和(sum: tmp1)与均值(mean: tmp2)\n",
    "    tmp1 = order.groupby(['air_store_id','visit_date'],\n",
    "        as_index=False)[['reserve_date_diff', 'reserve_visitors']].sum().rename(\n",
    "        columns={'visit_date':'visit_date', 'reserve_date_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = order.groupby(['air_store_id','visit_date'],\n",
    "        as_index=False)[['reserve_date_diff', 'reserve_visitors']].mean().rename(\n",
    "        columns={'visit_date':'visit_date', 'reserve_date_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    order = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "    return order\n",
    "\n",
    "\n",
    "# 以(air_store_id, dow)为分组，计算visitors的最小值，均值，中位数，最大值，样本大小\n",
    "# 主要计算了时间（dow）相关的信息\n",
    "def store_x_dow(tra, tes):\n",
    "    '''\n",
    "    Return:\n",
    "    -----\n",
    "    `air_store_id`, `dow`, ...\n",
    "    '''\n",
    "    unique_stores = tes['air_store_id'].unique()\n",
    "    stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores,\n",
    "                                      'dow': [i]*len(unique_stores)}) for i in range(7)],\n",
    "                        axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    tmp = tra.groupby(['air_store_id','dow']).agg(\n",
    "        {'visitors': [np.min,np.mean,np.median,np.max,np.size]}).reset_index()\n",
    "    tmp.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors',\n",
    "                   'median_visitors','max_visitors','count_observations']\n",
    "    stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "    return stores\n",
    "\n",
    "\n",
    "def genre_feats(store_data):\n",
    "    '''\n",
    "    Return:\n",
    "    -----\n",
    "    `air_store_id`, ...\n",
    "    '''\n",
    "    store_info = store_data[['air_store_id', 'air_genre_name']].copy()\n",
    "    store_info['air_genre_name'] = store_info['air_genre_name'].map(\n",
    "        lambda x: str(str(x).replace('/',' ')))\n",
    "\n",
    "    lbl = LabelEncoder()\n",
    "    max_genre = np.max((store_info['air_genre_name'].str.split().apply(lambda x: len(x))))\n",
    "    for i in range(max_genre):\n",
    "        store_info['air_genre_name'+str(i)] = lbl.fit_transform(store_info['air_genre_name'].map(\n",
    "            lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    # store_info['air_genre_name'] = lbl.fit_transform(store_info['air_genre_name'])  # 对genre_name进行直接编码\n",
    "    return store_info\n",
    "\n",
    "\n",
    "def area_feats(store_data):\n",
    "    '''\n",
    "    Return:\n",
    "    -----\n",
    "    `air_store_id`, ...\n",
    "    '''\n",
    "    store_info = store_data[['air_store_id', 'air_area_name', 'latitude', 'longitude']].copy()\n",
    "    # 区域名称特征\n",
    "    # store_info['air_area_name'] = store_info['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))  # 把'-' 换成空格' '\n",
    "    store_info['air_area_alias'] = store_info['air_area_name'].map(lambda x: re.sub('(-\\w+?|\\d)', '', x))  # 把\"-xxx\"或者数字换成空字符串\n",
    "\n",
    "    lbl = LabelEncoder()\n",
    "    max_area = np.max((store_info['air_area_name'].str.split().apply(lambda x: len(x))))\n",
    "    for i in range(max_area):\n",
    "        store_info['air_area_name'+str(i)] = lbl.fit_transform(store_info['air_area_alias'].map(\n",
    "            lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    # store_info['air_area_name'] = lbl.fit_transform(store_info['air_area_name'])  # 对area_name进行直接编码\n",
    "    store_info.drop('air_area_alias', axis=1, inplace=True)\n",
    "\n",
    "    # 区域坐标特征\n",
    "    # 经纬度特征\n",
    "    store_info['var_max_lat'] = store_info['latitude'].max() - store_info['latitude']\n",
    "    store_info['var_max_long'] = store_info['longitude'].max() - store_info['longitude']\n",
    "    # NEW FEATURES FROM Georgii Vyshnia\n",
    "    # 经度 + 纬度?\n",
    "    store_info['lon_plus_lat'] = store_info['longitude'] + store_info['latitude']\n",
    "\n",
    "    return store_info\n",
    "\n",
    "\n",
    "def holiday_feats(holiday):\n",
    "    '''\n",
    "    Return:\n",
    "    -----\n",
    "    `visit_date`, ...\n",
    "    '''\n",
    "    tmp = holiday.drop('day_of_week', axis=1)\n",
    "    # 周末的holiday_flg置为0\n",
    "    tmp.loc[(tmp['visit_date'].dt.dayofweek>4) & tmp['holiday_flg']==1, :] = 0\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def mean_avg(tra, hol):\n",
    "    '''\n",
    "    Return:\n",
    "    -----\n",
    "    `air_store_id`, `dow`, `holiday_flg`, `visitors_mv`\n",
    "    '''\n",
    "    air_visit_data = tra.copy()\n",
    "    date_info = hol.copy()\n",
    "    # 把周末的holiday_flag置为0\n",
    "    date_info.loc[(date_info['visit_date'].dt.dayofweek>4) & date_info['holiday_flg']==1, :] = 0\n",
    "\n",
    "    # 根据hol.index计算日期对应的权重weight\n",
    "    date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5\n",
    "\n",
    "    # 在visit_data中匹配日期weight\n",
    "    visit_data = air_visit_data.merge(date_info, on='visit_date', how='left')\n",
    "\n",
    "    # 将访问量转化为对数访问量log1p\n",
    "    visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "    # 按照 air_store_id, dow, holiday_flg 的分组计算加权平均\n",
    "    visitors = visit_data.groupby(['air_store_id', 'dow', 'holiday_flg']).apply(\n",
    "        lambda x:( (x['weight'] * x['visitors']).sum() / x['weight'].sum() )).reset_index()\n",
    "    visitors.rename(columns={0:'visitors_mv'}, inplace=True) \n",
    "    return visitors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>year</th>\n",
       "      <th>dow</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>doy</th>\n",
       "      <th>date_int</th>\n",
       "      <th>air_genre_name</th>\n",
       "      <th>air_area_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>20160113</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20160114</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20160115</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20160116</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>20160118</td>\n",
       "      <td>Dining bar</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id visit_date  visitors  year  dow  month  season  week  \\\n",
       "0  air_ba937bf13d40fb24 2016-01-13        25  2016    2      1       1     2   \n",
       "1  air_ba937bf13d40fb24 2016-01-14        32  2016    3      1       1     2   \n",
       "2  air_ba937bf13d40fb24 2016-01-15        29  2016    4      1       1     2   \n",
       "3  air_ba937bf13d40fb24 2016-01-16        22  2016    5      1       1     2   \n",
       "4  air_ba937bf13d40fb24 2016-01-18         6  2016    0      1       1     3   \n",
       "\n",
       "   doy  date_int air_genre_name                 air_area_name  \n",
       "0   13  20160113     Dining bar  Tōkyō-to Minato-ku Shibakōen  \n",
       "1   14  20160114     Dining bar  Tōkyō-to Minato-ku Shibakōen  \n",
       "2   15  20160115     Dining bar  Tōkyō-to Minato-ku Shibakōen  \n",
       "3   16  20160116     Dining bar  Tōkyō-to Minato-ku Shibakōen  \n",
       "4   18  20160118     Dining bar  Tōkyō-to Minato-ku Shibakōen  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tra'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set features.\n",
      "Processing Order data...\n",
      "Processing Store X dow...\n",
      "Processing Genre_feats...\n",
      "Processing Area_feats...\n",
      "Processing Holiday_feats...\n",
      "Test set features.\n",
      "Processing Order data...\n",
      "Processing Store X dow...\n",
      "Processing Genre_feats...\n",
      "Processing Area_feats...\n",
      "Processing Holiday_feats...\n",
      "Processing Total_order_feats...\n",
      "Processing lat&lon_feats...\n"
     ]
    }
   ],
   "source": [
    "# 增加时间特征\n",
    "# train = time_feats(data['tra'])\n",
    "# test = time_feats(data['tes'])\n",
    "\n",
    "train = data['tra'].drop(['air_genre_name', 'air_area_name'], axis=1).copy()\n",
    "test = data['tes'].drop(['air_genre_name', 'air_area_name'], axis=1).copy()\n",
    "\n",
    "'''\n",
    "for df in [train, test]:\n",
    "    print(\"Processing Order data...\")\n",
    "    df = pd.merge(df, order_feats(data['ar']), how='left', on=['air_store_id', 'visit_date'])\n",
    "    df = pd.merge(df, order_feats(data['hr']), how='left', on=['air_store_id', 'visit_date'])\n",
    "    print(\"Processing Store X dow...\")\n",
    "    df = pd.merge(df, store_x_dow(data['tra'], data['tes']), how='left', on=['air_store_id', 'dow'])\n",
    "    print(\"Processing Genre_feats...\")\n",
    "    df = pd.merge(df, genre_feats(data['as']), how='left', on=['air_store_id'])\n",
    "    print(\"Processing Area_feats...\")\n",
    "    df = pd.merge(df, area_feats(data['as']), how='left', on=['air_store_id'])\n",
    "    print(\"Processing Holiday_feats...\")\n",
    "    df = pd.merge(df, holiday_feats(data['hol']), how='left', on=['visit_date'])\n",
    "    df.fillna(-1)\n",
    "'''\n",
    "\n",
    "print(\"Train set features.\")\n",
    "print(\"Processing Order data...\")\n",
    "train = pd.merge(train, order_feats(data['ar']), how='left', on=['air_store_id', 'visit_date'])\n",
    "train = pd.merge(train, order_feats(data['hr']), how='left', on=['air_store_id', 'visit_date'])\n",
    "print(\"Processing Store X dow...\")\n",
    "train = pd.merge(train, store_x_dow(data['tra'], data['tes']), how='left', on=['air_store_id', 'dow'])\n",
    "print(\"Processing Genre_feats...\")\n",
    "train = pd.merge(train, genre_feats(data['as']), how='left', on=['air_store_id'])\n",
    "print(\"Processing Area_feats...\")\n",
    "train = pd.merge(train, area_feats(data['as']), how='left', on=['air_store_id'])\n",
    "print(\"Processing Holiday_feats...\")\n",
    "train = pd.merge(train, holiday_feats(data['hol']), how='left', on=['visit_date'])\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(\"Test set features.\")\n",
    "print(\"Processing Order data...\")\n",
    "test = pd.merge(test, order_feats(data['ar']), how='left', on=['air_store_id', 'visit_date'])\n",
    "test = pd.merge(test, order_feats(data['hr']), how='left', on=['air_store_id', 'visit_date'])\n",
    "print(\"Processing Store X dow...\")\n",
    "test = pd.merge(test, store_x_dow(data['tra'], data['tes']), how='left', on=['air_store_id', 'dow'])\n",
    "print(\"Processing Genre_feats...\")\n",
    "test = pd.merge(test, genre_feats(data['as']), how='left', on=['air_store_id'])\n",
    "print(\"Processing Area_feats...\")\n",
    "test = pd.merge(test, area_feats(data['as']), how='left', on=['air_store_id'])\n",
    "print(\"Processing Holiday_feats...\")\n",
    "test = pd.merge(test, holiday_feats(data['hol']), how='left', on=['visit_date'])\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "# 增加预订总和以及预订平均值特征\n",
    "print(\"Processing Total_order_feats...\")\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# 经纬度特征\n",
    "print(\"Processing lat&lon_feats...\")\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "# 经度 + 纬度?\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude']\n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_a</th>\n",
       "      <th>mean_a</th>\n",
       "      <th>std_a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_area_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Daimyō</th>\n",
       "      <td>408708</td>\n",
       "      <td>20.667914</td>\n",
       "      <td>16.846033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Hakata Ekimae</th>\n",
       "      <td>106994</td>\n",
       "      <td>21.149239</td>\n",
       "      <td>14.904164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Imaizumi</th>\n",
       "      <td>7644</td>\n",
       "      <td>15.925000</td>\n",
       "      <td>9.101297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Momochi</th>\n",
       "      <td>41036</td>\n",
       "      <td>20.365261</td>\n",
       "      <td>19.039513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fukuoka-ken Fukuoka-shi Shiobaru</th>\n",
       "      <td>37400</td>\n",
       "      <td>17.163837</td>\n",
       "      <td>15.372278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sum_a     mean_a      std_a\n",
       "air_area_name                                                      \n",
       "Fukuoka-ken Fukuoka-shi Daimyō         408708  20.667914  16.846033\n",
       "Fukuoka-ken Fukuoka-shi Hakata Ekimae  106994  21.149239  14.904164\n",
       "Fukuoka-ken Fukuoka-shi Imaizumi         7644  15.925000   9.101297\n",
       "Fukuoka-ken Fukuoka-shi Momochi         41036  20.365261  19.039513\n",
       "Fukuoka-ken Fukuoka-shi Shiobaru        37400  17.163837  15.372278"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_agg_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 匹配Count & Agg特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = train.merge(\n",
    "  store_agg_all,          how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  store_mean_std_monthly, how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  store_agg_monthly,      how='left', left_on=['air_store_id', 'month'],           right_index=True).merge(\n",
    "  store_mean_std_weekly,  how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  store_agg_weekly,       how='left', left_on=['air_store_id', 'week'],            right_index=True).merge(\n",
    "  store_agg_dow,          how='left', left_on=['air_store_id', 'dow'],             right_index=True).merge(\n",
    "  store_penetration,      how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  genre_agg_all,          how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  genre_mean_std_monthly, how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  genre_agg_monthly,      how='left', left_on=['air_genre_name', 'month'],         right_index=True).merge(\n",
    "  genre_mean_std_weekly,  how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  genre_agg_weekly,       how='left', left_on=['air_genre_name', 'week'],          right_index=True).merge(\n",
    "  genre_agg_dow,          how='left', left_on=['air_genre_name', 'dow'],           right_index=True).merge(\n",
    "  genre_penetration,      how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  area_agg_all,           how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  area_mean_std_monthly,  how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  area_agg_monthly,       how='left', left_on=['air_area_name', 'month'],          right_index=True).merge(\n",
    "  area_mean_std_weekly,   how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  area_agg_weekly,        how='left', left_on=['air_area_name', 'week'],           right_index=True).merge(\n",
    "  area_agg_dow,           how='left', left_on=['air_area_name', 'dow'],            right_index=True).merge(\n",
    "  area_penetration,       how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  genre_area_cnt_all,     how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_mean_std_monthly, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_agg_monthly, how='left', left_on=['air_genre_name', 'air_area_name', 'month'], right_index=True).merge(\n",
    "  genre_area_mean_std_weekly, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_agg_weekly, how='left', left_on=['air_genre_name', 'air_area_name', 'week'], right_index=True).merge(\n",
    "  genre_area_agg_dow, how='left', left_on=['air_genre_name', 'air_area_name', 'dow'], right_index=True).merge(\n",
    "  genre_area_penetration, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_diver, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True)\n",
    "\n",
    "test = test.merge(\n",
    "  store_agg_all,          how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  store_mean_std_monthly, how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  store_agg_monthly,      how='left', left_on=['air_store_id', 'month'],           right_index=True).merge(\n",
    "  store_mean_std_weekly,  how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  store_agg_weekly,       how='left', left_on=['air_store_id', 'week'],            right_index=True).merge(\n",
    "  store_agg_dow,          how='left', left_on=['air_store_id', 'dow'],             right_index=True).merge(\n",
    "  store_penetration,      how='left', left_on='air_store_id',                      right_index=True).merge(\n",
    "  genre_agg_all,          how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  genre_mean_std_monthly, how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  genre_agg_monthly,      how='left', left_on=['air_genre_name', 'month'],         right_index=True).merge(\n",
    "  genre_mean_std_weekly,  how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  genre_agg_weekly,       how='left', left_on=['air_genre_name', 'week'],          right_index=True).merge(\n",
    "  genre_agg_dow,          how='left', left_on=['air_genre_name', 'dow'],           right_index=True).merge(\n",
    "  genre_penetration,      how='left', left_on='air_genre_name',                    right_index=True).merge(\n",
    "  area_agg_all,           how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  area_mean_std_monthly,  how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  area_agg_monthly,       how='left', left_on=['air_area_name', 'month'],          right_index=True).merge(\n",
    "  area_mean_std_weekly,   how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  area_agg_weekly,        how='left', left_on=['air_area_name', 'week'],           right_index=True).merge(\n",
    "  area_agg_dow,           how='left', left_on=['air_area_name', 'dow'],            right_index=True).merge(\n",
    "  area_penetration,       how='left', left_on='air_area_name',                     right_index=True).merge(\n",
    "  genre_area_cnt_all,     how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_mean_std_monthly, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_agg_monthly, how='left', left_on=['air_genre_name', 'air_area_name', 'month'], right_index=True).merge(\n",
    "  genre_area_mean_std_weekly, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_agg_weekly, how='left', left_on=['air_genre_name', 'air_area_name', 'week'], right_index=True).merge(\n",
    "  genre_area_agg_dow, how='left', left_on=['air_genre_name', 'air_area_name', 'dow'], right_index=True).merge(\n",
    "  genre_area_penetration, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True).merge(\n",
    "  genre_area_diver, how='left', left_on=['air_genre_name', 'air_area_name'], right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in train.dtypes.loc[train.isnull().any(axis=0)].index:\n",
    "    train.loc[:, idx].fillna(train[idx].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in test.dtypes.loc[test.isnull().any(axis=0)].index:\n",
    "    test.loc[:, idx].fillna(test[idx].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 移动平均预测特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mean Average feats...\n",
      "\tProcessing Train set\n",
      "\tProcessing Test set\n"
     ]
    }
   ],
   "source": [
    "# 添加移动平均预测特征\n",
    "print(\"Processing Mean Average feats...\")\n",
    "visitors_mv = mean_avg(data['tra'], data['hol'])\n",
    "\n",
    "print(\"\\tProcessing Train set\")\n",
    "train = pd.merge(train, visitors_mv, how='left', on=['air_store_id', 'dow', 'holiday_flg'])\n",
    "miss_idx = train['visitors_mv'].isnull()\n",
    "train.loc[miss_idx, 'visitors_mv'] = train[miss_idx].merge(visitors_mv.loc[visitors_mv.holiday_flg==0, ['air_store_id', 'dow', 'visitors_mv']],\n",
    "                                                           on=('air_store_id', 'dow'), how='left')['visitors_mv_y'].values\n",
    "miss_idx = train['visitors_mv'].isnull()\n",
    "train.loc[miss_idx, 'visitors_mv'] = train[miss_idx].merge(visitors_mv[['air_store_id', 'visitors_mv']].groupby('air_store_id').mean().reset_index(),\n",
    "                                                           on='air_store_id', how='left')['visitors_mv_y'].values\n",
    "\n",
    "print(\"\\tProcessing Test set\")\n",
    "test = pd.merge(test, visitors_mv, how='left', on=['air_store_id', 'dow', 'holiday_flg'])\n",
    "miss_idx = test['visitors_mv'].isnull()\n",
    "test.loc[miss_idx, 'visitors_mv'] = test[miss_idx].merge(visitors_mv.loc[visitors_mv.holiday_flg==0, ['air_store_id', 'dow', 'visitors_mv']],\n",
    "                                                           on=('air_store_id', 'dow'), how='left')['visitors_mv_y'].values\n",
    "miss_idx = test['visitors_mv'].isnull()\n",
    "test.loc[miss_idx, 'visitors_mv'] = test[miss_idx].merge(visitors_mv[['air_store_id', 'visitors_mv']].groupby('air_store_id').mean().reset_index(),\n",
    "                                                           on='air_store_id', how='left')['visitors_mv_y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                object\n",
       "air_store_id      object\n",
       "air_genre_name    object\n",
       "air_area_name     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes[test.dtypes == 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准化 & 变量转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>year</th>\n",
       "      <th>dow</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>doy</th>\n",
       "      <th>date_int</th>\n",
       "      <th>...</th>\n",
       "      <th>max_ga_d</th>\n",
       "      <th>std_ga_d</th>\n",
       "      <th>len_ga_d</th>\n",
       "      <th>penet_ga</th>\n",
       "      <th>store_num</th>\n",
       "      <th>area_store_num</th>\n",
       "      <th>genre_store_num</th>\n",
       "      <th>area_store_ratio</th>\n",
       "      <th>genre_store_ratio</th>\n",
       "      <th>visitors_mv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>20160113</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>11.643092</td>\n",
       "      <td>399.0</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>2.915902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20160114</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>11.576332</td>\n",
       "      <td>400.0</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>2.739233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>20160115</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>14.187465</td>\n",
       "      <td>404.0</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>3.519119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>2016</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20160116</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12.351608</td>\n",
       "      <td>401.0</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>3.184821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>20160118</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>9.264949</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.009538</td>\n",
       "      <td>8.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>2.200934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id visit_date  visitors  year  dow  month  season  week  \\\n",
       "0  air_ba937bf13d40fb24 2016-01-13        25  2016    2      1       1     2   \n",
       "1  air_ba937bf13d40fb24 2016-01-14        32  2016    3      1       1     2   \n",
       "2  air_ba937bf13d40fb24 2016-01-15        29  2016    4      1       1     2   \n",
       "3  air_ba937bf13d40fb24 2016-01-16        22  2016    5      1       1     2   \n",
       "4  air_ba937bf13d40fb24 2016-01-18         6  2016    0      1       1     3   \n",
       "\n",
       "   doy  date_int     ...       max_ga_d   std_ga_d  len_ga_d  penet_ga  \\\n",
       "0   13  20160113     ...           73.0  11.643092     399.0  0.009538   \n",
       "1   14  20160114     ...           79.0  11.576332     400.0  0.009538   \n",
       "2   15  20160115     ...           64.0  14.187465     404.0  0.009538   \n",
       "3   16  20160116     ...           67.0  12.351608     401.0  0.009538   \n",
       "4   18  20160118     ...           47.0   9.264949     322.0  0.009538   \n",
       "\n",
       "   store_num  area_store_num  genre_store_num  area_store_ratio  \\\n",
       "0        8.0            51.0            108.0          0.156863   \n",
       "1        8.0            51.0            108.0          0.156863   \n",
       "2        8.0            51.0            108.0          0.156863   \n",
       "3        8.0            51.0            108.0          0.156863   \n",
       "4        8.0            51.0            108.0          0.156863   \n",
       "\n",
       "   genre_store_ratio  visitors_mv  \n",
       "0           0.074074     2.915902  \n",
       "1           0.074074     2.739233  \n",
       "2           0.074074     3.519119  \n",
       "3           0.074074     3.184821  \n",
       "4           0.074074     2.200934  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Predict #0\n",
    "\n",
    "先用xgboost或lightGBM生成单模型，便于开展特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=500, score=0.5249333841439336, total= 7.8min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=500 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=500, score=0.5179823522122393, total= 7.7min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=500 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 16.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=500, score=0.5248134699758493, total= 7.7min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=500, score=0.5247639688304637, total= 7.7min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=500, score=0.5286246057994941, total= 7.7min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=800, score=0.5156181160183296, total=12.4min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=800, score=0.5092455206554998, total=12.4min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=800, score=0.5148675905898945, total=12.4min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=800, score=0.5158033445349568, total=12.3min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=800, score=0.5189095503378124, total=12.3min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=1000, score=0.5133456357128559, total=15.4min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=1000, score=0.5070662495111329, total=15.4min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=1000, score=0.5126689441582892, total=15.4min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=1000, score=0.5136932375720387, total=15.5min\n",
      "[CV] learning_rate=0.3, max_depth=12, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=12, n_estimators=1000, score=0.5168585112177833, total=15.4min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=500, score=0.5181651797259805, total=10.2min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=500, score=0.5156898062839861, total=10.2min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=500, score=0.5191403301489702, total=10.2min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=500, score=0.5179322300679456, total=10.2min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=500, score=0.5236545644727297, total=10.2min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=800, score=0.5179088136954053, total=16.1min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=800, score=0.5153788873367138, total=16.1min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=800, score=0.5188564368218782, total=16.0min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=800, score=0.5176969639148217, total=16.1min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=800, score=0.5233469925633332, total=16.0min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=1000, score=0.5179068004715808, total=18.1min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=1000, score=0.5153735784656598, total=18.1min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=1000, score=0.518853418779897, total=17.9min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=1000, score=0.5176950382533527, total=18.0min\n",
      "[CV] learning_rate=0.3, max_depth=15, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=15, n_estimators=1000, score=0.5233415543217552, total=18.2min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=500, score=0.5279504376783352, total=10.3min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=500, score=0.5170481110367178, total=10.2min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=500, score=0.5246914628854187, total=10.3min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=500, score=0.5251982881244681, total=10.5min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=500 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=500, score=0.5303057979247288, total=10.5min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=800, score=0.5279504186288653, total=12.1min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=800, score=0.5170464972071815, total=12.4min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=800, score=0.524689265293294, total=11.9min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=800, score=0.5251969971114296, total=12.1min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=800 ...............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=800, score=0.530305036568198, total=12.5min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=1000, score=0.5279503344296854, total=13.3min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=1000, score=0.5170468177810547, total=13.3min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=1000, score=0.5246897930024464, total=13.3min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=1000, score=0.5251968916284634, total=13.5min\n",
      "[CV] learning_rate=0.3, max_depth=20, n_estimators=1000 ..............\n",
      "[CV]  learning_rate=0.3, max_depth=20, n_estimators=1000, score=0.5303042832797793, total=13.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 617.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nscores = []\\nk_fold = KFold(n_splits=5, shuffle=True)\\nfor train_idx, test_idx in k_fold.split(train):\\n    X_train, X_test = train.loc[train_idx, col], train.loc[test_idx, col]\\n    y_train, y_test = np.log1p(train.loc[train_idx, 'visitors'].values), train.loc[test_idx, 'visitors'].values\\n    model0.fit(X_train, y_train)\\n    y_pred = np.expm1(model0.predict(X_test))\\n    scores.append(rmsle(y_pred, y_test))\\nprint(scores)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmsle(h, y): \n",
    "    \"\"\"\n",
    "    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n",
    "    \n",
    "    Args:\n",
    "        h - numpy array containing predictions with shape (n_samples, n_targets)\n",
    "        y - numpy array containing targets with shape (n_samples, n_targets)\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n",
    "\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "\n",
    "param_grid = [\n",
    "  {'learning_rate': [0.3], 'n_estimators': [500, 800, 1000], 'max_depth': [12, 15, 20]}\n",
    " ]\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "model0 = GridSearchCV(estimator=XGBRegressor(eval_metric='rmse', subsample=0.6, colsample_bytree=0.8, max_depth =10, nthread=3),\n",
    "                      param_grid=param_grid,\n",
    "                      cv=k_fold,\n",
    "                      verbose=3)\n",
    "model0.fit(train[col], np.log1p(train.loc[:, 'visitors'].values))\n",
    "\n",
    "'''\n",
    "scores = []\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "for train_idx, test_idx in k_fold.split(train):\n",
    "    X_train, X_test = train.loc[train_idx, col], train.loc[test_idx, col]\n",
    "    y_train, y_test = np.log1p(train.loc[train_idx, 'visitors'].values), train.loc[test_idx, 'visitors'].values\n",
    "    model0.fit(X_train, y_train)\n",
    "    y_pred = np.expm1(model0.predict(X_test))\n",
    "    scores.append(rmsle(y_pred, y_test))\n",
    "print(scores)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, eval_metric='rmse', gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=3, objective='reg:linear', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.6),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'learning_rate': [0.001, 0.05, 0.1, 0.3], 'n_estimators': range(100, 501, 100), 'max_depth': [5, 10]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model0.plk']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model0, 'model0.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model0 = joblib.load('model0.plk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_result = model0.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_learning_rate', 'param_max_depth', 'param_n_estimators', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score', 'split0_train_score', 'split1_train_score', 'split2_train_score', 'split3_train_score', 'split4_train_score', 'mean_train_score', 'std_train_score'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  455.84074426,   728.50306282,   909.95217662,   602.68544073,\n",
       "         944.26814952,  1063.90308566,   611.65678129,   719.41170115,\n",
       "         792.1493957 ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.get('mean_fit_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(cv_result.get('mean_test_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52422354,  0.5148888 ,  0.5127265 ,  0.51891641,  0.5186376 ,\n",
       "        0.51863406,  0.5250388 ,  0.52503762,  0.5250376 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_result.get('mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.expm1(model0.predict(test[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub = test[['id', 'visitors']]\n",
    "sub['visitors'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sub['visitors'] = sub['visitors'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  visitors\n",
       "0  air_00a91d42b08b08d9_2017-04-23         1\n",
       "1  air_00a91d42b08b08d9_2017-04-24        15\n",
       "2  air_00a91d42b08b08d9_2017-04-25        33\n",
       "3  air_00a91d42b08b08d9_2017-04-26        21\n",
       "4  air_00a91d42b08b08d9_2017-04-27        39"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission0107-04.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Predict #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 辅助计时函数\n",
    "def time_cnt(delta):\n",
    "    total_secs = int(delta.total_seconds())\n",
    "    format = '{h}H: {m}M: {s}S'\n",
    "    h, m = total_secs // 3600, total_secs % 3600\n",
    "    m, s = m // 60, m % 60\n",
    "    return format.format(h=h, m=m, s=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling...\n",
      "Training...\n",
      "Time consume:  0H: 2M: 42S\n",
      "RMSE GradientBoostingRegressor:  0.45636955229\n",
      "RMSE KNeighborsRegressor:  0.482428864514\n",
      "Time consume:  0H: 3M: 29S\n"
     ]
    }
   ],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors', 'air_genre_name', 'air_area_name']]\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5\n",
    "\n",
    "time0 = datetime.now()\n",
    "print(\"Modeling...\")\n",
    "model1 = GradientBoostingRegressor(learning_rate=0.2, random_state=3)\n",
    "model2 = KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
    "\n",
    "print(\"Training...\")\n",
    "model1.fit(train[col], np.log1p(train['visitors'].values))\n",
    "print(\"Time consume: \", time_cnt(datetime.now()-time0))\n",
    "time0 = datetime.now()\n",
    "model2.fit(train[col], np.log1p(train['visitors'].values))\n",
    "\n",
    "visitors_pred1 = model1.predict(train[col])\n",
    "visitors_pred2 = model2.predict(train[col])\n",
    "\n",
    "print('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), visitors_pred1))\n",
    "print('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), visitors_pred2))\n",
    "print(\"Time consume: \", time_cnt(datetime.now()-time0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Predict #2 Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>dow</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>visitors_mv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.203625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.091042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.325868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.353439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.475056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  dow  holiday_flg  visitors_mv\n",
       "0  air_00a91d42b08b08d9    0          0.0     3.203625\n",
       "1  air_00a91d42b08b08d9    0          1.0     3.091042\n",
       "2  air_00a91d42b08b08d9    1          0.0     3.325868\n",
       "3  air_00a91d42b08b08d9    2          0.0     3.353439\n",
       "4  air_00a91d42b08b08d9    3          0.0     3.475056"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitors_mv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_avg(tra, hol):\n",
    "    air_visit_data = tra.copy()\n",
    "    date_info = hol.copy()\n",
    "    # 把周末的holiday_flag置为0\n",
    "    date_info.loc[(date_info['visit_date'].dt.dayofweek>4) & date_info['holiday_flg']==1, :] = 0\n",
    "\n",
    "    # 根据hol.index计算日期对应的权重weight\n",
    "    date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5\n",
    "\n",
    "    # 在visit_data中匹配日期weight\n",
    "    visit_data = air_visit_data.merge(date_info, on='visit_date', how='left')\n",
    "\n",
    "    # 将访问量转化为对数访问量log1p\n",
    "    visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "    # 按照 air_store_id, dow, holiday_flg 的分组计算加权平均\n",
    "    visitors = visit_data.groupby(['air_store_id', 'dow', 'holiday_flg']).apply(\n",
    "        lambda x:( (x['weight'] * x['visitors']).sum() / x['weight'].sum() )).reset_index()\n",
    "    visitors.rename(columns={0:'visitors'}, inplace=True) \n",
    "    return visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>dow</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>visitors_mv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.203625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.091042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  dow  holiday_flg  visitors_mv\n",
       "0  air_00a91d42b08b08d9    0          0.0     3.203625\n",
       "1  air_00a91d42b08b08d9    0          1.0     3.091042"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitors_mv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visitors</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>dow</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>rs1_x</th>\n",
       "      <th>rv1_x</th>\n",
       "      <th>...</th>\n",
       "      <th>air_area_name5</th>\n",
       "      <th>air_area_name6</th>\n",
       "      <th>var_max_lat</th>\n",
       "      <th>var_max_long</th>\n",
       "      <th>lon_plus_lat</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>total_reserv_sum</th>\n",
       "      <th>total_reserv_mean</th>\n",
       "      <th>total_reserv_dt_diff_mean</th>\n",
       "      <th>visitors_mv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.326629</td>\n",
       "      <td>4.519803</td>\n",
       "      <td>175.447598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.326629</td>\n",
       "      <td>4.519803</td>\n",
       "      <td>175.447598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.203625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-25</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.326629</td>\n",
       "      <td>4.519803</td>\n",
       "      <td>175.447598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.325868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.326629</td>\n",
       "      <td>4.519803</td>\n",
       "      <td>175.447598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.353439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.326629</td>\n",
       "      <td>4.519803</td>\n",
       "      <td>175.447598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.475056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  visitors visit_date          air_store_id  \\\n",
       "0  air_00a91d42b08b08d9_2017-04-23         0 2017-04-23  air_00a91d42b08b08d9   \n",
       "1  air_00a91d42b08b08d9_2017-04-24         0 2017-04-24  air_00a91d42b08b08d9   \n",
       "2  air_00a91d42b08b08d9_2017-04-25         0 2017-04-25  air_00a91d42b08b08d9   \n",
       "3  air_00a91d42b08b08d9_2017-04-26         0 2017-04-26  air_00a91d42b08b08d9   \n",
       "4  air_00a91d42b08b08d9_2017-04-27         0 2017-04-27  air_00a91d42b08b08d9   \n",
       "\n",
       "   dow  month  season  week  rs1_x  rv1_x     ...       air_area_name5  \\\n",
       "0    6      4       2    16    0.0    0.0     ...                    0   \n",
       "1    0      4       2    17    0.0    0.0     ...                    0   \n",
       "2    1      4       2    17    0.0    0.0     ...                    0   \n",
       "3    2      4       2    17    0.0    0.0     ...                    0   \n",
       "4    3      4       2    17    0.0    0.0     ...                    0   \n",
       "\n",
       "   air_area_name6  var_max_lat  var_max_long  lon_plus_lat  holiday_flg  \\\n",
       "0               0     8.326629      4.519803    175.447598          0.0   \n",
       "1               0     8.326629      4.519803    175.447598          0.0   \n",
       "2               0     8.326629      4.519803    175.447598          0.0   \n",
       "3               0     8.326629      4.519803    175.447598          0.0   \n",
       "4               0     8.326629      4.519803    175.447598          0.0   \n",
       "\n",
       "   total_reserv_sum  total_reserv_mean  total_reserv_dt_diff_mean  visitors_mv  \n",
       "0               0.0                0.0                        0.0     1.098612  \n",
       "1               0.0                0.0                        0.0     3.203625  \n",
       "2               0.0                0.0                        0.0     3.325868  \n",
       "3               0.0                0.0                        0.0     3.353439  \n",
       "4               0.0                0.0                        0.0     3.475056  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = test[['id','visitors']].copy()\n",
    "sub1['visitors'] = (model1.predict(test[col]) + model2.predict(test[col])) / 2\n",
    "sub1['visitors'] = np.expm1(sub1['visitors']).clip(lower=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub2 = test[['id', 'visitors_mv']].copy()\n",
    "sub2['visitors'] = sub2['visitors_mv'].map(pd.np.expm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visitors_mv</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>3.203625</td>\n",
       "      <td>23.621632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
       "      <td>3.325868</td>\n",
       "      <td>26.823130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
       "      <td>3.353439</td>\n",
       "      <td>27.600920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
       "      <td>3.475056</td>\n",
       "      <td>31.299646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  visitors_mv   visitors\n",
       "0  air_00a91d42b08b08d9_2017-04-23     1.098612   2.000000\n",
       "1  air_00a91d42b08b08d9_2017-04-24     3.203625  23.621632\n",
       "2  air_00a91d42b08b08d9_2017-04-25     3.325868  26.823130\n",
       "3  air_00a91d42b08b08d9_2017-04-26     3.353439  27.600920\n",
       "4  air_00a91d42b08b08d9_2017-04-27     3.475056  31.299646"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('submission%s.csv' % datetime.now().strftime('%M%S'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
