{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "99b96bae-2969-439e-95d7-e7f59504eeb2",
    "_execution_state": "idle",
    "_uuid": "851c5a0960ad8bcd9ee95a7681e08abf422efd41"
   },
   "source": [
    "This script is translate from @Fabienvs's [R code](https://www.kaggle.com/fabienvs/instacart-xgboost-starter-lb-0-3791), I think it may help kagglers who do not use R.  \n",
    "I really appreciate @Fabienvs's great work, to be honest, I have no idea about how to handling this kind of problem(this is the first time I encounter recommendation problem- -)  \n",
    "here we go!! \n",
    "below exist some very useful functions I write by my own, you can download from my [github repo](https://github.com/NickYi1990/Kaggle_Buddy.git)\n",
    "sorry for adding some Chinese in it, it would not affect the code\n",
    "## The dataset is too big, you should run it on your desktop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e1d5f7cc-e69b-4b75-9e49-c42ab70e1f81",
    "_execution_state": "idle",
    "_uuid": "39008a2aa2a73398cb80d8b683bb626b902893e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import gc\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "4449943f-7006-4c6f-a7b4-4bfc24dc1f1a",
    "_execution_state": "idle",
    "_uuid": "043ac17afde77d99e4e69565d3f4c090a6e797bd"
   },
   "outputs": [],
   "source": [
    "def load_data(path_data):\n",
    "    '''\n",
    "    --------------------------------order_product--------------------------------\n",
    "    * Unique in order_id + product_id\n",
    "    '''\n",
    "    priors = pd.read_csv(path_data + 'order_products__prior.csv', \n",
    "                     dtype={\n",
    "                            'order_id': np.int32,\n",
    "                            'product_id': np.uint16,\n",
    "                            'add_to_cart_order': np.int16,\n",
    "                            'reordered': np.int8})\n",
    "    train = pd.read_csv(path_data + 'order_products__train.csv', \n",
    "                    dtype={\n",
    "                            'order_id': np.int32,\n",
    "                            'product_id': np.uint16,\n",
    "                            'add_to_cart_order': np.int16,\n",
    "                            'reordered': np.int8})\n",
    "    '''\n",
    "    --------------------------------order--------------------------------\n",
    "    * This file tells us which set (prior, train, test) an order belongs\n",
    "    * Unique in order_id\n",
    "    * order_id in train, prior, test has no intersection\n",
    "    * this is the #order_number order of this user\n",
    "    '''\n",
    "    orders = pd.read_csv(path_data + 'orders.csv', \n",
    "                         dtype={\n",
    "                                'order_id': np.int32,\n",
    "                                'user_id': np.int64,\n",
    "                                'eval_set': 'category',\n",
    "                                'order_number': np.int16,\n",
    "                                'order_dow': np.int8,\n",
    "                                'order_hour_of_day': np.int8,\n",
    "                                'days_since_prior_order': np.float32})\n",
    "\n",
    "    #  order in prior, train, test has no duplicate\n",
    "    #  order_ids_pri = priors.order_id.unique()\n",
    "    #  order_ids_trn = train.order_id.unique()\n",
    "    #  order_ids_tst = orders[orders.eval_set == 'test']['order_id'].unique()\n",
    "    #  print(set(order_ids_pri).intersection(set(order_ids_trn)))\n",
    "    #  print(set(order_ids_pri).intersection(set(order_ids_tst)))\n",
    "    #  print(set(order_ids_trn).intersection(set(order_ids_tst)))\n",
    "\n",
    "    '''\n",
    "    --------------------------------product--------------------------------\n",
    "    * Unique in product_id\n",
    "    '''\n",
    "    products = pd.read_csv(path_data + 'products.csv')\n",
    "    aisles = pd.read_csv(path_data + \"aisles.csv\")\n",
    "    departments = pd.read_csv(path_data + \"departments.csv\")\n",
    "    sample_submission = pd.read_csv(path_data + \"sample_submission.csv\")\n",
    "    \n",
    "    return priors, train, orders, products, aisles, departments, sample_submission\n",
    "\n",
    "class tick_tock:\n",
    "    def __init__(self, process_name, verbose=1):\n",
    "        self.process_name = process_name\n",
    "        self.verbose = verbose\n",
    "    def __enter__(self):\n",
    "        if self.verbose:\n",
    "            print(self.process_name + \" begin ......\")\n",
    "            self.begin_time = time.time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.verbose:\n",
    "            end_time = time.time()\n",
    "            print(self.process_name + \" end ......\")\n",
    "            print('time lapsing {0} s \\n'.format(end_time - self.begin_time))\n",
    "            \n",
    "def ka_add_groupby_features_1_vs_n(df, group_columns_list, agg_dict, only_new_feature=True):\n",
    "    '''Create statistical columns, group by [N columns] and compute stats on [N column]\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       df: pandas dataframe\n",
    "          Features matrix\n",
    "       group_columns_list: list_like\n",
    "          List of columns you want to group with, could be multiple columns\n",
    "       agg_dict: python dictionary\n",
    "\n",
    "       Return\n",
    "       ------\n",
    "       new pandas dataframe with original columns and new added columns\n",
    "\n",
    "       Example\n",
    "       -------\n",
    "       {real_column_name: {your_specified_new_column_name : method}}\n",
    "       agg_dict = {'user_id':{'prod_tot_cnts':'count'},\n",
    "                   'reordered':{'reorder_tot_cnts_of_this_prod':'sum'},\n",
    "                   'user_buy_product_times': {'prod_order_once':lambda x: sum(x==1),\n",
    "                                              'prod_order_more_than_once':lambda x: sum(x==2)}}\n",
    "       ka_add_stats_features_1_vs_n(train, ['product_id'], agg_dict)\n",
    "    '''\n",
    "    with tick_tock(\"add stats features\"):\n",
    "        try:\n",
    "            if type(group_columns_list) == list:\n",
    "                pass\n",
    "            else:\n",
    "                raise TypeError(k + \"should be a list\")\n",
    "        except TypeError as e:\n",
    "            print(e)\n",
    "            raise\n",
    "\n",
    "        df_new = df.copy()\n",
    "        grouped = df_new.groupby(group_columns_list)\n",
    "\n",
    "        the_stats = grouped.agg(agg_dict)\n",
    "        the_stats.columns = the_stats.columns.droplevel(0)\n",
    "        the_stats.reset_index(inplace=True)\n",
    "        if only_new_feature:\n",
    "            df_new = the_stats\n",
    "        else:\n",
    "            df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n",
    "\n",
    "    return df_new\n",
    "\n",
    "def ka_add_groupby_features_n_vs_1(df, group_columns_list, target_columns_list, methods_list, keep_only_stats=True, verbose=1):\n",
    "    '''Create statistical columns, group by [N columns] and compute stats on [1 column]\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       df: pandas dataframe\n",
    "          Features matrix\n",
    "       group_columns_list: list_like\n",
    "          List of columns you want to group with, could be multiple columns\n",
    "       target_columns_list: list_like\n",
    "          column you want to compute stats, need to be a list with only one element\n",
    "       methods_list: list_like\n",
    "          methods that you want to use, all methods that supported by groupby in Pandas\n",
    "\n",
    "       Return\n",
    "       ------\n",
    "       new pandas dataframe with original columns and new added columns\n",
    "\n",
    "       Example\n",
    "       -------\n",
    "       ka_add_stats_features_n_vs_1(train, group_columns_list=['x0'], target_columns_list=['x10'])\n",
    "    '''\n",
    "    with tick_tock(\"add stats features\", verbose):\n",
    "        dicts = {\"group_columns_list\": group_columns_list , \"target_columns_list\": target_columns_list, \"methods_list\" :methods_list}\n",
    "\n",
    "        for k, v in dicts.items():\n",
    "            try:\n",
    "                if type(v) == list:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise TypeError(k + \"should be a list\")\n",
    "            except TypeError as e:\n",
    "                print(e)\n",
    "                raise\n",
    "\n",
    "        grouped_name = ''.join(group_columns_list)\n",
    "        target_name = ''.join(target_columns_list)\n",
    "        combine_name = [[grouped_name] + [method_name] + [target_name] for method_name in methods_list]\n",
    "\n",
    "        df_new = df.copy()\n",
    "        grouped = df_new.groupby(group_columns_list)\n",
    "\n",
    "        the_stats = grouped[target_name].agg(methods_list).reset_index()\n",
    "        the_stats.columns = [grouped_name] + \\\n",
    "                            ['_%s_%s_by_%s' % (grouped_name, method_name, target_name) \\\n",
    "                             for (grouped_name, method_name, target_name) in combine_name]\n",
    "        if keep_only_stats:\n",
    "            return the_stats\n",
    "        else:\n",
    "            df_new = pd.merge(left=df_new, right=the_stats, on=group_columns_list, how='left')\n",
    "        return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "13b01c67-a0f3-412a-ab62-b49390a494bf",
    "_execution_state": "idle",
    "_uuid": "652ba1d76cc52f7bb8992bc002cd18ced9155ab5"
   },
   "outputs": [],
   "source": [
    "path_data = '../input/'\n",
    "priors, train, orders, products, aisles, departments, sample_submission = load_data(path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "818ba5dc-48df-4e67-bc80-853a97265964",
    "_execution_state": "idle",
    "_uuid": "d0a7c5706e2fe9e65d9f1f87ae4a6c817bca0049"
   },
   "source": [
    "# Product part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d93e83d-8531-43b7-9e1a-5543a350393c",
    "_execution_state": "idle",
    "_uuid": "c9a8c9f490b127ec40674c4e63c2558a09c49c5e"
   },
   "outputs": [],
   "source": [
    "# Products information ----------------------------------------------------------------\n",
    "# add order information to priors set\n",
    "priors_orders_detail = orders.merge(right=priors, how='inner', on='order_id')\n",
    "\n",
    "# create new variables\n",
    "# _user_buy_product_times: 用户是第几次购买该商品\n",
    "priors_orders_detail.loc[:,'_user_buy_product_times'] = priors_orders_detail.groupby(['user_id', 'product_id']).cumcount() + 1\n",
    "# _prod_tot_cnts: 该商品被购买的总次数,表明被喜欢的程度\n",
    "# _reorder_tot_cnts_of_this_prod: 这件商品被再次购买的总次数\n",
    "### 我觉得下面两个很不好理解，考虑改变++++++++++++++++++++++++++\n",
    "# _prod_order_once: 该商品被购买一次的总次数\n",
    "# _prod_order_more_than_once: 该商品被购买一次以上的总次数\n",
    "agg_dict = {'user_id':{'_prod_tot_cnts':'count'}, \n",
    "            'reordered':{'_prod_reorder_tot_cnts':'sum'}, \n",
    "            '_user_buy_product_times': {'_prod_buy_first_time_total_cnt':lambda x: sum(x==1),\n",
    "                                        '_prod_buy_second_time_total_cnt':lambda x: sum(x==2)}}\n",
    "prd = ka_add_groupby_features_1_vs_n(priors_orders_detail, ['product_id'], agg_dict)\n",
    "\n",
    "# _prod_reorder_prob: 这个指标不好理解\n",
    "# _prod_reorder_ratio: 商品复购率\n",
    "prd['_prod_reorder_prob'] = prd._prod_buy_second_time_total_cnt / prd._prod_buy_first_time_total_cnt\n",
    "prd['_prod_reorder_ratio'] = prd._prod_reorder_tot_cnts / prd._prod_tot_cnts\n",
    "prd['_prod_reorder_times'] = 1 + prd._prod_reorder_tot_cnts / prd._prod_buy_first_time_total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "876b1e85-103a-4f02-bc82-32c66345be48",
    "_execution_state": "idle",
    "_uuid": "22faa543fec15643fd2e357306b909ceba38bd93"
   },
   "outputs": [],
   "source": [
    "prd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e57204c-f54e-4f6b-8962-9a1a86391df8",
    "_execution_state": "idle",
    "_uuid": "e6bb6010c3d614f8a61cd8cfc0cafdf91f60ac98"
   },
   "source": [
    "# User Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0d7e8b5d-96f2-4fde-b915-2e4eb5948747",
    "_execution_state": "busy",
    "_uuid": "2caadb329254e7e3f20383847e0d75485662a913"
   },
   "outputs": [],
   "source": [
    "# _user_total_orders: 用户的总订单数\n",
    "# 可以考虑加入其它统计指标++++++++++++++++++++++++++\n",
    "# _user_sum_days_since_prior_order: 距离上次购买时间(和),这个只能在orders表里面计算，priors_orders_detail不是在order level上面unique\n",
    "# _user_mean_days_since_prior_order: 距离上次购买时间(均值)\n",
    "agg_dict_2 = {'order_number':{'_user_total_orders':'max'},\n",
    "              'days_since_prior_order':{'_user_sum_days_since_prior_order':'sum', \n",
    "                                        '_user_mean_days_since_prior_order': 'mean'}}\n",
    "users = ka_add_groupby_features_1_vs_n(orders[orders.eval_set == 'prior'], ['user_id'], agg_dict_2)\n",
    "\n",
    "# _user_reorder_ratio: reorder的总次数 / 第一单后买后的总次数\n",
    "# _user_total_products: 用户购买的总商品数\n",
    "# _user_distinct_products: 用户购买的unique商品数\n",
    "agg_dict_3 = {'reordered':\n",
    "              {'_user_reorder_ratio': \n",
    "               lambda x: sum(priors_orders_detail.ix[x.index,'reordered']==1)/\n",
    "                         sum(priors_orders_detail.ix[x.index,'order_number'] > 1)},\n",
    "              'product_id':{'_user_total_products':'count', \n",
    "                            '_user_distinct_products': lambda x: x.nunique()}}\n",
    "us = ka_add_groupby_features_1_vs_n(priors_orders_detail, ['user_id'], agg_dict_3)\n",
    "users = users.merge(us, how='inner')\n",
    "\n",
    "# 平均每单的商品数\n",
    "# 每单中最多的商品数，最少的商品数++++++++++++++\n",
    "users['_user_average_basket'] = users._user_total_products / users._user_total_orders\n",
    "\n",
    "us = orders[orders.eval_set != \"prior\"][['user_id', 'order_id', 'eval_set', 'days_since_prior_order']]\n",
    "us.rename(index=str, columns={'days_since_prior_order': 'time_since_last_order'}, inplace=True)\n",
    "\n",
    "users = users.merge(us, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "10f6d636-8ca5-4c75-ac38-ee2e9a2d4a76",
    "_execution_state": "busy",
    "_uuid": "9747c3e86c1eafbd0601879c667274e3d7b96e78"
   },
   "outputs": [],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "97ff9b86-233c-437f-987f-28fda714a064",
    "_execution_state": "idle",
    "_uuid": "a7001f50583b0f5f0297f8018ff8fcbf566255db"
   },
   "source": [
    "# Database Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3659d59a-f2b5-442a-85e4-99ed46844d9e",
    "_execution_state": "busy",
    "_uuid": "7958608441e2cc92445368f7ab7183280ac8d7f1"
   },
   "outputs": [],
   "source": [
    "# 这里应该还有很多变量可以被添加\n",
    "# _up_order_count: 用户购买该商品的次数\n",
    "# _up_first_order_number: 用户第一次购买该商品所处的订单数\n",
    "# _up_last_order_number: 用户最后一次购买该商品所处的订单数\n",
    "# _up_average_cart_position: 该商品被添加到购物篮中的平均位置\n",
    "agg_dict_4 = {'order_number':{'_up_order_count': 'count', \n",
    "                              '_up_first_order_number': 'min', \n",
    "                              '_up_last_order_number':'max'}, \n",
    "              'add_to_cart_order':{'_up_average_cart_position': 'mean'}}\n",
    "\n",
    "data = ka_add_groupby_features_1_vs_n(df=priors_orders_detail, \n",
    "                                                      group_columns_list=['user_id', 'product_id'], \n",
    "                                                      agg_dict=agg_dict_4)\n",
    "\n",
    "data = data.merge(prd, how='inner', on='product_id').merge(users, how='inner', on='user_id')\n",
    "# 该商品购买次数 / 总的订单数\n",
    "# 最近一次购买商品 - 最后一次购买该商品\n",
    "# 该商品购买次数 / 第一次购买该商品到最后一次购买商品的的订单数\n",
    "data['_up_order_rate'] = data._up_order_count / data._user_total_orders\n",
    "data['_up_order_since_last_order'] = data._user_total_orders - data._up_last_order_number\n",
    "data['_up_order_rate_since_first_order'] = data._up_order_count / (data._user_total_orders - data._up_first_order_number + 1)\n",
    "\n",
    "# add user_id to train set\n",
    "train = train.merge(right=orders[['order_id', 'user_id']], how='left', on='order_id')\n",
    "data = data.merge(train[['user_id', 'product_id', 'reordered']], on=['user_id', 'product_id'], how='left')\n",
    "\n",
    "# release Memory\n",
    "# del train, prd, users\n",
    "# gc.collect()\n",
    "# release Memory\n",
    "del priors_orders_detail, orders\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9206cd4-138b-417a-9ec8-9b703bc89286",
    "_execution_state": "busy",
    "_uuid": "ffe7006fd12c8faf2463e7781a49d01b66b366cc"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f016091-8029-4c63-a029-2829720e8890",
    "_execution_state": "idle",
    "_uuid": "81180a81e251ab065267a13fc9c62cd8c021b823"
   },
   "source": [
    "# Create Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7e211223-ce8d-4cfc-a9a3-48e6b70a9341",
    "_execution_state": "busy",
    "_uuid": "95989c977ec21519088e084c7b88cc89def05b1b"
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "train = data.loc[data.eval_set == \"train\",:]\n",
    "train.drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis=1, inplace=True)\n",
    "train.loc[:, 'reordered'] = train.reordered.fillna(0)\n",
    "\n",
    "X_test = data.loc[data.eval_set == \"test\",:]\n",
    "\n",
    "# subsample 让training时间更短\n",
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('reordered', axis=1), train.reordered,\n",
    "                                                    test_size=0.9, random_state=42)\n",
    "d_train = xgboost.DMatrix(X_train, y_train)\n",
    "xgb_params = {\n",
    "    \"objective\"         : \"reg:logistic\"\n",
    "    ,\"eval_metric\"      : \"logloss\"\n",
    "    ,\"eta\"              : 0.1\n",
    "    ,\"max_depth\"        : 6\n",
    "    ,\"min_child_weight\" :10\n",
    "    ,\"gamma\"            :0.70\n",
    "    ,\"subsample\"        :0.76\n",
    "    ,\"colsample_bytree\" :0.95\n",
    "    ,\"alpha\"            :2e-05\n",
    "    ,\"lambda\"           :10\n",
    "}\n",
    "\n",
    "watchlist= [(d_train, \"train\")]\n",
    "bst = xgboost.train(params=xgb_params, dtrain=d_train, num_boost_round=80, evals=watchlist, verbose_eval=10)\n",
    "xgboost.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e17b046b-9165-4ca8-86ea-371f4092615a",
    "_execution_state": "busy",
    "_uuid": "d3d69878acb6d40f75c0ca499ac34a1879ad54f5"
   },
   "outputs": [],
   "source": [
    "d_test = xgboost.DMatrix(X_test.drop(['eval_set', 'user_id', 'order_id', 'reordered', 'product_id'], axis=1))\n",
    "X_test.loc[:,'reordered'] = (bst.predict(d_test) > 0.21).astype(int)\n",
    "X_test.loc[:, 'product_id'] = X_test.product_id.astype(str)\n",
    "submit = ka_add_groupby_features_n_vs_1(X_test[X_test.reordered == 1], \n",
    "                                               group_columns_list=['order_id'],\n",
    "                                               target_columns_list= ['product_id'],\n",
    "                                               methods_list=[lambda x: ' '.join(set(x))], keep_only_stats=True)\n",
    "submit.columns = sample_submission.columns.tolist()\n",
    "submit_final = sample_submission[['order_id']].merge(submit, how='left').fillna('None')\n",
    "submit_final.to_csv(\"python_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
